{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我: 你是谁\n",
      "机器人: 我是你的小可爱\n",
      "我: 你猜猜我是谁\n",
      "机器人: 你猜我猜不猜\n",
      "我: 你猜猜我是谁\n",
      "机器人: 你猜我猜不猜\n",
      "我: \n",
      "机器人: 你猜我猜不猜\n",
      "我: \n",
      "机器人: 你猜我猜不猜\n",
      "我: \n",
      "机器人: 你猜我猜不猜\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     temp_sentence \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m我:\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m     sentence \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (temp_sentence \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sentence) \u001b[39m>\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m     19\u001b[0m         \u001b[39m#由于该模型输入最大长度为300，避免长度超出限制长度过长需要进行裁剪\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1203\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1206\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1207\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gpt_model import GPT\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chooseEpoch = 12 # \n",
    "    modelPath = 'models/GPT2_epoch'+str(chooseEpoch)+'.pt'\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    model = GPT().to(device)\n",
    "    model.load_state_dict(torch.load(modelPath))\n",
    "    model.eval()\n",
    "    \n",
    "    #初始输入是空，每次加上后面的对话信息\n",
    "    sentence = ''\n",
    "    while True:\n",
    "        temp_sentence = input(\"我:\")\n",
    "        sentence += (temp_sentence + '\\t')\n",
    "        if len(sentence) > 200:\n",
    "            #由于该模型输入最大长度为300，避免长度超出限制长度过长需要进行裁剪\n",
    "            t_index = sentence.find('\\t')\n",
    "            sentence = sentence[t_index + 1:]\n",
    "        print(\"我:\", temp_sentence)\n",
    "        print(\"机器人:\", model.answer(sentence))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Evaluation:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['那', '个', '饭', '凉', '了', '吧', '唧', '的', '怎', '么', '吃', '啊', '摔']], [['看', '了', '下', '全', '文', '，', '那', '女', '的', '考', '试', '当', '天', '就', '表', '明', '身', '体', '不', '舒', '服', '了', '，', '考', '试', '不', '是', '她', '预', '约', '是', '教', '练', '自', '己', '安', '排', '的', '，', '教', '练', '还', '让', '她', '考', '试', '不', '就', '是', '教', '练', '的', '错', '吗', '？', '而', '且', '她', '住', '院', '花', '了', '3', '1', '万', '，', '赔', '3', '0', '万', '不', '过', '分', '吧']], [['更', '改', '要', '提', '前', '3', '天', '，', '当', '天', '不', '上', '场', '视', '为', '放', '弃', '考', '试', '，', '又', '要', '重', '新', '报', '名', '重', '新', '交', '钱', '，', '估', '计', '她', '也', '不', '肯', '的', '吧', '？']], [['好', '好', '好', '，', '偶', '遇', '我', '大', '闺', '蜜']], [['嗯', '，', '私', '聊', '吧', '，', '拜', '拜']]]\n",
      "[['我', '也', '是', '，', '我', '也', '是', '，', '我', '们', '这', '边', '的', '虾', '米', '都', '是', '，', '我', '们', '这', '边', '的', '虾', '米', '都', '是', '，', '我', '们', '这', '边', '的', '虾', '米', '都', '是', '，', '我', '们', '这', '边', '的', '虾', '米', '都', '是', '，', '我', '们', '这', '边', '的', '虾', '米', '都', '是', '，', '我', '们', '这', '边', '的', '虾', '米', '都', '是', '，', '虾', '米', '都', '是', '，', '虾', '米', '都', '是', '，', '虾', '米', '都', '是', '，', '虾', '米', '都', '是', '，', '虾', '米', '都', '是', '，', '虾', '米', '都', '是', '，', '虾', '米', '都'], ['你', '这', '是', '在', '说', '我', '吗', '？'], ['我', '们', '这', '边', '的', '教', '练', '都', '是', '教', '练', '的', '，', '我', '们', '这', '边', '教', '练', '说', '她', '不', '要', '钱', '，', '我', '们', '这', '边', '教', '练', '说', '，', '我', '们', '这', '边', '教', '练', '不', '让', '，', '我', '们', '这', '边', '教', '练', '说', '，', '我', '们', '这', '边', '教', '练', '说', '，', '我', '们', '这', '边', '教', '练', '说', '，', '我', '们', '这', '边', '教', '练', '说', '，', '我', '们', '这', '边', '教', '练', '说', '，', '我', '们', '这', '边', '教', '练', '说', '，', '我', '们', '这', '边', '教', '练', '说', '，', '我', '们', '这'], ['我', '也', '是', '，', '我', '在', '家', '里', '，', '你', '在', '哪', '里'], ['你', '是', '大', '闺', '蜜']]\n",
      " | Time: 0m 17s\n",
      "\tBLEUScore= 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ubuntu-user/.local/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def getQuesGoldAns(validQAPath):\n",
    "    Questions = []\n",
    "    GoldenAnswers = []\n",
    "    with open(validQAPath, 'r', encoding='utf-8') as src_file:\n",
    "        for line in src_file:\n",
    "            QA = line.strip().split('\\t')\n",
    "            Questions.append(QA[0])\n",
    "\n",
    "            ga = QA[1]\n",
    "            ga = list(ga)\n",
    "            ga = \" \".join(ga)\n",
    "            ga = [wordpunct_tokenize(ga)]\n",
    "            GoldenAnswers.append(ga)\n",
    "\n",
    "    return Questions,GoldenAnswers\n",
    "\n",
    "def GPT_BLUE_Evaluation(GPTModelPath, QAPath):\n",
    "    device = torch.device('cuda')\n",
    "    model = GPT().to(device)\n",
    "    model.load_state_dict(torch.load(GPTModelPath))\n",
    "    model.eval()\n",
    "\n",
    "    Questions,GoldenAnswers = getQuesGoldAns(QAPath)\n",
    "    ModelAnswers = []\n",
    "    for Q in tqdm(Questions):\n",
    "        A = model.answer(Q+'\\t').strip()\n",
    "        A = list(A)\n",
    "        A = \" \".join(A)\n",
    "        A = wordpunct_tokenize(A)\n",
    "        ModelAnswers.append(A)\n",
    "    \n",
    "    # print(GoldenAnswers[:5])\n",
    "    # print(ModelAnswers[:5])\n",
    "\n",
    "    bleu_score = corpus_bleu(GoldenAnswers, ModelAnswers) # 接下来要使这个东西连续三次不上升就跳出循环\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaEpochList = [6]\n",
    "    for e in evaEpochList:\n",
    "        chooseEpoch = e # \n",
    "        modelPath = 'models/GPT2_epoch'+str(chooseEpoch)+'.pt'\n",
    "        validQAPath = 'dataset/QA_valid_100.txt'\n",
    "\n",
    "        device = torch.device('cuda')\n",
    "        model = GPT().to(device)\n",
    "        model.load_state_dict(torch.load(modelPath))\n",
    "        model.eval()\n",
    "\n",
    "        # evaluation\n",
    "        print(\"Epoch %d Evaluation:\"%(chooseEpoch), end='')\n",
    "        start_time = time.time()\n",
    "        Questions,GoldenAnswers = getQuesGoldAns(validQAPath)\n",
    "        ModelAnswers = []\n",
    "        for Q in tqdm(Questions):\n",
    "            A = model.answer(Q+'\\t').strip()\n",
    "            A = list(A)\n",
    "            A = \" \".join(A)\n",
    "            A = wordpunct_tokenize(A)\n",
    "            ModelAnswers.append(A)\n",
    "        \n",
    "        print(GoldenAnswers[:5])\n",
    "        print(ModelAnswers[:5])\n",
    "\n",
    "        bleu_score = corpus_bleu(GoldenAnswers, ModelAnswers) # 接下来要使这个东西连续三次不上升就跳出循环\n",
    "        end_time = time.time()\n",
    "        eva_mins, eva_secs = epoch_time(start_time, end_time)\n",
    "        print(f' | Time: {eva_mins}m {eva_secs}s')\n",
    "        print(\"\\tBLEUScore= %f\"%(bleu_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Model Answers in QAData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Ans show:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask: 啊我好爱虾仁蛋黄酱金枪鱼蛋黄酱\n",
      "ModelAns: 我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我也是，我\n",
      "Ask: 考试撞墙关驾校屁事？你怎么不顺便把考场施工单位也告了？\n",
      "ModelAns: 你这是在黑河南的节奏吗？\n",
      "Ask: 看了下全文，那女的考试当天就表明身体不舒服了，考试不是她预约是教练自己安排的，教练还让她考试不就是教练的错吗？而且她住院花了31万，赔30万不过分吧\n",
      "ModelAns: 我们这边的考试不是她的错，她是她的错，她的错，她的错，她的错，她的错，她的错，她的错，她的错，她的错，她的错，她的错，她的错，她的错，她的惩罚她的惩罚她的惩罚她的惩罚她的惩罚她的惩罚她的惩罚她的惩罚她的\n",
      "Ask: 我先去穿衣服，准备走了\n",
      "ModelAns: 我也想去\n",
      "Ask: 好好好，偶遇我大闺蜜\n",
      "ModelAns: 我也是，我们一起去的\n",
      "Ask: 期待小猎豹的表现\n",
      "ModelAns: 我也是\n",
      "Ask: 为了你们新年有惊喜，我也是用心良苦了\n",
      "ModelAns: 新年快乐\n",
      "Ask: 世界上最远的距离，不是爱，不是恨，而是熟悉的人，渐渐变得陌生。\n",
      "ModelAns: 是的，我们都在，我们在的是不是爱，但是我们在的是不是爱，我们在的是不是爱，我们在的是不是爱，我们在的是不是爱，我们在的是不是爱，我们在的是不是爱，我们在的是不是爱，我们在的是不是爱，我们在的是不是爱，我\n",
      "Ask: 生日快乐！年年18\n",
      "ModelAns: 谢谢\n",
      "Ask: 你是18那我17好了\n",
      "ModelAns: 我17\n",
      "Ask: 青青妹！！！好久不见！别来无恙！！假期也没见你呜呜呜！我不想你，谁想你谁小狗呜呜呜呜呜呜呜！汪！\n",
      "ModelAns: 我也是！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "Ask: 文艺小青年\n",
      "ModelAns: 你是不是傻\n",
      "Ask: 你知道这期录的什么时候播不\n",
      "ModelAns: 不知道\n",
      "Ask: 不知道\n",
      "ModelAns: 我也是\n",
      "Ask: 好吧，我去建房子了\n",
      "ModelAns: 你去建房子干嘛\n",
      "Ask: 啧啧\n",
      "ModelAns: 你不是也没睡\n",
      "Ask: 我都建两天了\n",
      "ModelAns: 我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这边也是，我们这\n",
      "Ask: 别再被烧了\n",
      "ModelAns: 我不会烧的\n",
      "Ask: 不会了都是防火的\n",
      "ModelAns: 我也是\n",
      "Ask: 北京欢迎你\n",
      "ModelAns: 我也想去\n",
      "Ask: 太原欢迎你~\n",
      "ModelAns: 你是哪个学校的？\n",
      "Ask: 早到早放假~\n",
      "ModelAns: 我们还有一个星期\n",
      "Ask: ………起码我现在可以欠下你\n",
      "ModelAns: 我也想要\n",
      "Ask: 那阿扎西也给我们发封包呗\n",
      "ModelAns: 我们这边有一个卖包的，我们这边没有\n",
      "Ask: 口袋没票子，怎么发\n",
      "ModelAns: 我也不知道，我是去看看\n",
      "Ask: 大饱眼福\n",
      "ModelAns: 你也是\n",
      "Ask: 有机会来南京\n",
      "ModelAns: 我在南京等你\n",
      "Ask: 介绍舍友么\n",
      "ModelAns: 我们宿舍的舍友\n",
      "Ask: 看第一张照片得反应，我没关注过任何女明星啊！怎么会有……\n",
      "ModelAns: 我也是第一次看到这种感觉，感觉好像是这样的\n",
      "Ask: 虫洞无处不在\n",
      "ModelAns: 我也是\n",
      "Ask: 可以收藏吗？\n",
      "ModelAns: 可以的\n",
      "Ask: 她生宝宝啦？恭喜呀\n",
      "ModelAns: 是啊，我们同一天生的\n",
      "Ask: 是啊，很快对不？帮忙想名字。\n",
      "ModelAns: 我也想知道，我们这边的人都是这样的，我们这边的人都是这样的，我们这边的人都是这样的，我们这边的人都是这样的。\n",
      "Ask: 槿，怎么样？\n",
      "ModelAns: 还行吧，就是不知道怎么样，我觉得挺好的，我觉得\n",
      "Ask: 打折打到亲妈都认不得了\n",
      "ModelAns: 我也是，我妈妈说我不要买，我就要买\n",
      "Ask: 居然！！！？？？\n",
      "ModelAns: 我们班的\n",
      "Ask: 对一件5折4件折上6折请问亲妈在那里\n",
      "ModelAns: 亲妈\n",
      "Ask: 这位同志你能不能有素质点，我们可是大学生啊\n",
      "ModelAns: 你们这些人，我们都素质低下的，你们素质低下的，我们素质低下的，你们素质低下的，我们素质低下的，你们素质低下的，我们素质低下的，你们素质低下的，我们素质低下的，你们素质低下的，我们素质低下的，你们素质低下\n",
      "Ask: 感谢校长能让我们看到今天的林彦俊\n",
      "ModelAns: 校友\n",
      "Ask: 我喜欢你呢套\n",
      "ModelAns: 我喜欢你\n",
      "Ask: 套？！！\n",
      "ModelAns: 是啊，我们学校的，你们学校的\n",
      "Ask: 电话套\n",
      "ModelAns: 我不是你的小公举了\n",
      "Ask: 我不用套呢你也可以买个试试\n",
      "ModelAns: 我买了个套，你买个试试\n",
      "Ask: 我们高中是跳小苹果，见过这样的学校吗\n",
      "ModelAns: 我们高中跳小苹果\n",
      "Ask: 以后不叫你早点休息了，无力......没用\n",
      "ModelAns: 我也是，我们都是夜猫子\n",
      "Ask: 别介样嘛！我只是失眠了。\n",
      "ModelAns: 我也是，失眠了\n",
      "Ask: 你这是读多少书啊\n",
      "ModelAns: 我们这是一个人的时候\n",
      "Ask: 因为过年期间有两个月都没读书，所以才读了几本，其实任务的真不多\n",
      "ModelAns: 我也是，一个月读一次，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，一年读书，\n",
      "Ask: 宁可食无肉，不可居无书\n",
      "ModelAns: 我不是一个人\n",
      "Ask: 当吃货遇见吃货...后果很严重。\n",
      "ModelAns: 我也是，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃货，吃\n",
      "Ask: 我翻到了\n",
      "ModelAns: 我翻到了\n",
      "Ask: 我都返到喇~\n",
      "ModelAns: 你几时返到喇？\n",
      "Ask: 阳台吗\n",
      "ModelAns: 是的，我们这边的阳光明媚，我们这边的阳光明媚，我们这边的阳光明媚\n",
      "Ask: 我爸说我怎么今天那么积极，那么早就回来了！\n",
      "ModelAns: 我也是，我爸说我不是一个人\n",
      "Ask: 哈哈因为吃货没有夜生活\n",
      "ModelAns: 我也是，我们都是吃货\n",
      "Ask: 冷冷的狗粮在脸上胡乱的拍。。\n",
      "ModelAns: 你是不是傻\n",
      "Ask: 你这么亲切的称呼他\n",
      "ModelAns: 我是说我的名字\n",
      "Ask: 这只是一句歌词。\n",
      "ModelAns: 我也是，我们这边的人都是这么说的，我们这边的人都是这么说的\n",
      "Ask: 不看此人故事，那给您换另外一人的故事吧！\n",
      "ModelAns: 我不知道啊，我只知道他们的故事\n",
      "Ask: 在哪里在哪里\n",
      "ModelAns: 在我心里\n",
      "Ask: 麦当劳阿\n",
      "ModelAns: 我们这边的小吃街都是这样的\n",
      "Ask: 没事慢慢来、缘分到了自然有\n",
      "ModelAns: 嗯嗯，我会的，你也要加油\n",
      "Ask: 闲的时候就特别想脱团\n",
      "ModelAns: 我也是\n",
      "Ask: 你现在很闲么==\n",
      "ModelAns: 我是闲的很闲\n",
      "Ask: 国庆假很闲，上学就又要忙了\n",
      "ModelAns: 我们也是，我们也是\n",
      "Ask: 老大做的什么梦啊\n",
      "ModelAns: 梦见我们在一起了\n",
      "Ask: 坐等李云迪的春晚节目啦\n",
      "ModelAns: 我也是，我们这里的春晚节目都是在湖南卫视播的\n",
      "Ask: 你要开面馆了？\n",
      "ModelAns: 我要吃面\n",
      "Ask: 你想表达什么朋友\n",
      "ModelAns: 我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是，我想说的是\n",
      "Ask: 没啥……\n",
      "ModelAns: 我也是，我们这边的人都是这样，我们这边的人都是这样，我们这边的人都是这样，我们这边的人都是这样，我们这边的人都是这样，我们这边的人都是这样，我们这边的人都是这样，我们这边的人都是这样，我们这边的人都是这\n",
      "Ask: 我都开学一个星期多了\n",
      "ModelAns: 我们也是，我们也是\n",
      "Ask: 幸好23号又可以放假了~\n",
      "ModelAns: 我们23号放假\n",
      "Ask: 我26号考完试才放假\n",
      "ModelAns: 我们26号考完\n",
      "Ask: 期待易烊千玺！！！\n",
      "ModelAns: 易烊千玺\n",
      "Ask: 你这是什么逻辑？\n",
      "ModelAns: 我是说我们的逻辑\n",
      "Ask: 我们猫猫的思想\n",
      "ModelAns: 我们猫猫的思想是不是有点不对\n",
      "Ask: 你是什么猫？啥大猫？\n",
      "ModelAns: 我是小猫\n",
      "Ask: 梁博的才华是大家有目共睹的\n",
      "ModelAns: 我们的大家都是大家的\n",
      "Ask: \"形影不离\"~美！\n",
      "ModelAns: 我也觉得美！\n",
      "Ask: 吸吸\n",
      "ModelAns: 吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸吸\n",
      "Ask: 哎~~~~青春无价\n",
      "ModelAns: 我们都老了\n",
      "Ask: 对啊，爱惜身体\n",
      "ModelAns: 我也是，我们都是\n",
      "Ask: 是啊！好好生活，有一个人爱的人就是最幸福的了\n",
      "ModelAns: 是啊，我们都要幸福\n",
      "Ask: 对的~有他就足够了\n",
      "ModelAns: 我也是，我们都是好朋友\n",
      "Ask: 是的！人生已足以~~~~哎~~~\n",
      "ModelAns: 我也是，我们都是好朋友\n",
      "Ask: 呵呵。好好找个呗\n",
      "ModelAns: 我不是一个人\n",
      "Ask: 画一个花边的被窝再画一个小伙陪着我\n",
      "ModelAns: 我也想要\n",
      "Ask: 先还钱\n",
      "ModelAns: 你不是说你不要我了吗\n",
      "Ask: 薛之谦动物世界\n",
      "ModelAns: 我不知道他是谁\n",
      "Ask: 教教我怎么长胖\n",
      "ModelAns: 我也不知道，我也不知道，我也不知道怎么长胖的\n",
      "Ask: 你转正了？\n",
      "ModelAns: 没有，我在家\n",
      "Ask: 晚上好\n",
      "ModelAns: 晚上好\n",
      "Ask: 您好\n",
      "ModelAns: 你好，我是你的小迷妹\n",
      "Ask: 在听千里你有听吗？\n",
      "ModelAns: 我有听过，不过我没听过，我也不知道为什么\n",
      "Ask: 是的，节目很不错！\n",
      "ModelAns: 是的，节目很好，但是节目很好，很好，很好，很好，很好\n",
      "Ask: 一个结束也是新的开始，小样，看好你呦，福州欢迎你常回来，哈哈哈\n",
      "ModelAns: 我也是福州的，我们这边的人都不知道有没有这个活动，我们这边的人都不知道有没有这个活动，我们这边的人都不知道有没有这个活动，我们这边的人都不知道有没有\n",
      "Ask: 啊～到时候去是不是要穿羽绒啦带暖宝宝啦～\n",
      "ModelAns: 带了，但是我不知道穿什么，我们这边也不冷，我们这边也不冷，我们这边也不冷，我们这边也不冷，我们这边也不冷，我们这边也不冷，我们这边也不冷，我们这边也不冷，我们这边也不冷，我们这边也不冷，我们这边也不冷，\n",
      "Ask: 必须的啊～～～你从何方来\n",
      "ModelAns: 我在家里\n",
      "Ask: 头发多久没减了\n",
      "ModelAns: 我也是，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我现在就是这样，我\n",
      "Ask: 解析星期天不交吧。。。\n",
      "ModelAns: 我们都是星期天交的\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch\n",
    "from gpt_model import GPT\n",
    "\n",
    "def getQuesGoldAns(validQAPath):\n",
    "    Questions = []\n",
    "    GoldenAnswers = []\n",
    "    with open(validQAPath, 'r', encoding='utf-8') as src_file:\n",
    "        for line in src_file:\n",
    "            QA = line.strip().split('\\t')\n",
    "            Questions.append(QA[0])\n",
    "            GoldenAnswers.append(QA[1])\n",
    "\n",
    "    return Questions,GoldenAnswers\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaEpochList = [12]\n",
    "    for e in evaEpochList:\n",
    "        chooseEpoch = e # \n",
    "        modelPath = 'models/GPT2_epoch'+str(chooseEpoch)+'.pt'\n",
    "        validQAPath = 'dataset/QA_valid_100.txt'\n",
    "\n",
    "        device = torch.device('cuda')\n",
    "        model = GPT().to(device)\n",
    "        model.load_state_dict(torch.load(modelPath))\n",
    "        model.eval()\n",
    "\n",
    "        # evaluation\n",
    "        print(\"Epoch %d Ans show:\"%(chooseEpoch), end='')\n",
    "        Questions,GoldenAnswers = getQuesGoldAns(validQAPath)\n",
    "        ModelAnswers = []\n",
    "        for Q in tqdm(Questions):\n",
    "            A = model.answer(Q+'\\t').strip()\n",
    "            ModelAnswers.append(A)\n",
    "        \n",
    "        for q,a in zip(Questions, ModelAnswers):\n",
    "            print(\"Ask: %s\\nModelAns: %s\"%(q,a))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION FOR BERT SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9871877431869507\n"
     ]
    }
   ],
   "source": [
    "# sentence-lv bert_score sample\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "ModelAnswers = ['我们都是星期天交的', '我们都是星期一交的']\n",
    "GoldenAnswers = ['我们都是星期天交的', '我们都是星期天交的']\n",
    "\n",
    "P, R, F1 = bert_score(ModelAnswers, GoldenAnswers, lang='zh')\n",
    "avg_bert_score = F1.mean().item()\n",
    "print(avg_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Evaluation:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2301/19008 [01:36<11:41, 23.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m ModelAnswers \u001b[39m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m Q \u001b[39min\u001b[39;00m tqdm(Questions):\n\u001b[0;32m---> 51\u001b[0m     A \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49manswer(Q\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     52\u001b[0m     ModelAnswers\u001b[39m.\u001b[39mappend(A)\n\u001b[1;32m     54\u001b[0m \u001b[39m# print(GoldenAnswers[:5])\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m# print(ModelAnswers[:5])\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[39m# 接下来要使这个东西连续三次不上升就跳出循环\u001b[39;00m\n",
      "File \u001b[0;32m~/mc36505nlp/GPT2_ChatRobot/gpt_model.py:224\u001b[0m, in \u001b[0;36mGPT.answer\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    220\u001b[0m dec_input \u001b[39m=\u001b[39m [word2id\u001b[39m.\u001b[39mget(word, \u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m word \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m word2id[\u001b[39m'\u001b[39m\u001b[39m<sep>\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence]\n\u001b[1;32m    222\u001b[0m dec_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(dec_input, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_decoder(dec_input)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    225\u001b[0m out \u001b[39m=\u001b[39m [id2word[\u001b[39mint\u001b[39m(\u001b[39mid\u001b[39m)] \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m output]\n\u001b[1;32m    226\u001b[0m \u001b[39m# 统计\"<sep>\"字符在结果中的索引\u001b[39;00m\n",
      "File \u001b[0;32m~/mc36505nlp/GPT2_ChatRobot/gpt_model.py:205\u001b[0m, in \u001b[0;36mGPT.greedy_decoder\u001b[0;34m(self, dec_input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     dec_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m    203\u001b[0m         [dec_input\u001b[39m.\u001b[39mdetach(), torch\u001b[39m.\u001b[39mtensor([[next_symbol]], dtype\u001b[39m=\u001b[39mdec_input\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mdevice)], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    204\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m dec_outputs, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(dec_input)\n\u001b[1;32m    206\u001b[0m projected \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(dec_outputs)\n\u001b[1;32m    207\u001b[0m prob \u001b[39m=\u001b[39m projected\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mc36505nlp/GPT2_ChatRobot/gpt_model.py:171\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, dec_inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m dec_self_attns \u001b[39m=\u001b[39m []\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    170\u001b[0m     \u001b[39m# dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     dec_outputs, dec_self_attn \u001b[39m=\u001b[39m layer(dec_outputs, dec_self_attn_mask)\n\u001b[1;32m    172\u001b[0m     dec_self_attns\u001b[39m.\u001b[39mappend(dec_self_attn)\n\u001b[1;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m dec_outputs, dec_self_attns\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mc36505nlp/GPT2_ChatRobot/gpt_model.py:140\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, dec_inputs, dec_self_attn_mask)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39mdec_inputs: [batch_size, tgt_len, d_model]\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39mdec_self_attn_mask: [batch_size, tgt_len, tgt_len]\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39m# dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m dec_outputs, dec_self_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n\u001b[1;32m    142\u001b[0m dec_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_ffn(dec_outputs)  \u001b[39m# [batch_size, tgt_len, d_model]\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m dec_outputs, dec_self_attn\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mc36505nlp/GPT2_ChatRobot/gpt_model.py:101\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, input_Q, input_K, input_V, attn_mask)\u001b[0m\n\u001b[1;32m     97\u001b[0m attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, n_heads, \u001b[39m1\u001b[39m,\n\u001b[1;32m     98\u001b[0m                                           \u001b[39m1\u001b[39m)  \u001b[39m# attn_mask : [batch_size, n_heads, seq_len, seq_len]\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m# context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m context, attn \u001b[39m=\u001b[39m ScaledDotProductAttention()(Q, K, V, attn_mask)\n\u001b[1;32m    102\u001b[0m context \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    103\u001b[0m                                           n_heads \u001b[39m*\u001b[39m d_v)  \u001b[39m# context: [batch_size, len_q, n_heads * d_v]\u001b[39;00m\n\u001b[1;32m    104\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(context)  \u001b[39m# [batch_size, len_q, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mc36505nlp/GPT2_ChatRobot/gpt_model.py:70\u001b[0m, in \u001b[0;36mScaledDotProductAttention.forward\u001b[0;34m(self, Q, K, V, attn_mask)\u001b[0m\n\u001b[1;32m     67\u001b[0m scores\u001b[39m.\u001b[39mmasked_fill_(attn_mask, \u001b[39m-\u001b[39m\u001b[39m1e9\u001b[39m)  \u001b[39m# Fills elements of self tensor with value where mask is True.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m attn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(scores)\n\u001b[0;32m---> 70\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attn, V)  \u001b[39m# [batch_size, n_heads, len_q, d_v]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m context, attn\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from bert_score import score as bert_score\n",
    "from gpt_model import GPT\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def getQuesGoldAns(validQAPath):\n",
    "    Questions = []\n",
    "    GoldenAnswers = []\n",
    "    with open(validQAPath, 'r', encoding='utf-8') as src_file:\n",
    "        for line in src_file:\n",
    "            QA = line.strip().split('\\t')\n",
    "            Questions.append(QA[0])\n",
    "            GoldenAnswers.append(QA[1])\n",
    "\n",
    "    return Questions,GoldenAnswers\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaEpochList = list(range(28,29))\n",
    "    \n",
    "    best_bert_score = -1\n",
    "    prev_bert_score = -1\n",
    "    no_improvement_count = 0\n",
    "    all_bert_score_evaluation_info = []\n",
    "\n",
    "    for e in evaEpochList:\n",
    "        chooseEpoch = e # \n",
    "        modelPath = 'models/GPT2_epoch'+str(chooseEpoch)+'.pt'\n",
    "        validQAPath = 'dataset/QA_test.txt'\n",
    "\n",
    "        device = torch.device('cuda')\n",
    "        model = GPT().to(device)\n",
    "        model.load_state_dict(torch.load(modelPath))\n",
    "        model.eval()\n",
    "\n",
    "        # evaluation\n",
    "        print(\"Epoch %d Evaluation:\"%(chooseEpoch), end='')\n",
    "        start_time = time.time()\n",
    "        Questions,GoldenAnswers = getQuesGoldAns(validQAPath) \n",
    "        ModelAnswers = []\n",
    "        for Q in tqdm(Questions):\n",
    "            A = model.answer(Q+'\\t').strip()\n",
    "            ModelAnswers.append(A)\n",
    "        \n",
    "        # print(GoldenAnswers[:5])\n",
    "        # print(ModelAnswers[:5])\n",
    "\n",
    "        # 接下来要使这个东西连续三次不上升就跳出循环\n",
    "        P, R, F1 = bert_score(ModelAnswers, GoldenAnswers, lang='zh')\n",
    "        avg_bert_score = F1.mean().item()\n",
    "        end_time = time.time()\n",
    "        eva_mins, eva_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if best_bert_score <= avg_bert_score:\n",
    "            best_bert_score = avg_bert_score\n",
    "\n",
    "        if prev_bert_score > avg_bert_score:\n",
    "            no_improvement_count += 1\n",
    "        else:\n",
    "            no_improvement_count = 0 \n",
    "\n",
    "        bert_score_evaluation_info = {\n",
    "            'epoch': chooseEpoch,\n",
    "            'bert_score': avg_bert_score,\n",
    "            'best_bert_score': best_bert_score,\n",
    "            'no_improvement_count': no_improvement_count\n",
    "        }\n",
    "\n",
    "        all_bert_score_evaluation_info.append(bert_score_evaluation_info)\n",
    "\n",
    "        with open('models/bert_score_evaluation_info.json', 'w') as json_file:\n",
    "            json.dump(all_bert_score_evaluation_info, json_file)\n",
    "\n",
    "        prev_bert_score = avg_bert_score\n",
    "\n",
    "        print(f' | Time: {eva_mins}m {eva_secs}s')\n",
    "        print(\"avg_bert_score= %f | best_bert_score= %f | no_improvement_count = %d\"%(avg_bert_score,best_bert_score,no_improvement_count))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc36505nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
